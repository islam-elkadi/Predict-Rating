{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a step-by-step tutorial on implementing Text Classification to solve a problem proposed by an IBM Data Science hackathon. The code demonstrated in this tutorial has been productionized and converted into a REST API. To view the code, please navigate to the <b>src</b> folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A client in the retail space has customer reviews on its products. Recently, the client has implemented a 5-star rating system as part of its reviews platform. However, that was note the case in the past, and due to that there remains large volumes of historical data that has not recieved 5-star ratings from its clients. This client has hired IBM to implement a text classification system that will predict the ratings for their customer historical reviews.\n",
    "\n",
    "The client has suggested that its competitor Amazon competes in the same space, and has plenty of open source data to use that contains ratings for similar products in this space. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from subprocess import run, PIPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join\n",
    "from os import remove, system, listdir, makedirs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dir(directory):\n",
    "    \"\"\"\n",
    "        Checks if directory doesn't exist, then creates it.\n",
    "\n",
    "        Paras:\n",
    "            directory: name of directory\n",
    "\n",
    "        Returns:\n",
    "            Boolean\n",
    "    \"\"\"\n",
    "    if not exists(directory): \n",
    "        makedirs(directory, exist_ok = True)\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Save data to directory**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(directory, name, docs, mode = \"w\"):\n",
    "    \"\"\"\n",
    "        Saves data to directory\n",
    "\n",
    "        Paras:\n",
    "            directory: directory to save data\n",
    "            name: name of file\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    make_dir(directory)\n",
    "    with open(join(directory, name), mode, encoding = \"utf-8\") as f:\n",
    "        f.write(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Store shell output to variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shell2var(cmd):\n",
    "    \"\"\"\n",
    "        Runs shell command and returns output in a varible\n",
    "\n",
    "        Paras:\n",
    "            cmd: shell command\n",
    "        Returns:\n",
    "            output of shell command\n",
    "    \"\"\"\n",
    "    result = run(args = cmd, stderr = PIPE, universal_newlines = True, stdout = PIPE, shell = True)   \n",
    "    return result.stdout "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot Histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_bar(dataframe, target, x_label, figsize, rotation = 0):\n",
    "    info = dataframe[target].value_counts()\n",
    "    info_id = info.index.tolist() \n",
    "    info_counts = info.tolist()\n",
    "    plt.figure(figsize = figsize)\n",
    "    plt.bar(info_id, info_counts, align = \"center\", color = \"green\")\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel('Counts')\n",
    "    plt.xticks(rotation = rotation)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtering unwanted columns**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_columns(dataframe, column_names):\n",
    "    \"\"\"\n",
    "        Filters out unwanted columns\n",
    "\n",
    "        Paras:\n",
    "            dataframe: dataframe to filter\n",
    "            column_names: columns to keep\n",
    "\n",
    "        Return:\n",
    "            df: filtered dataframe\n",
    "    \"\"\"\n",
    "    try:\n",
    "        dataframe = dataframe[column_names]\n",
    "        dataframe = dataframe.dropna()\n",
    "        return dataframe.reset_index(drop = True)\n",
    "    except (ValueError, KeyError):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merging Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_workbooks(dataframes, column_names):\n",
    "    \"\"\"\n",
    "        Merges multiple workbooks based on column names\n",
    "\n",
    "        Paras:\n",
    "            dataframes: dfs to merge\n",
    "            Column_names: column names to merge on\n",
    "        Returns:\n",
    "            df: concatenated dataframes\n",
    "    \"\"\"\n",
    "    return pd.concat([remove_columns(df, column_names) for df in dataframes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsample Reviews**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DownSampleReviews(split_reviews, column_names):\n",
    "    \"\"\"\n",
    "        Down-samples the reviews such that there are equal number of labels\n",
    "        Paras:\n",
    "            split_reviews : list of dataframes, where each dataframe is filtered by label\n",
    "            column_names  : column names to keep for balanced dataframe \n",
    "        Returns:\n",
    "            Balanced dataframe\n",
    "    \"\"\"\n",
    "    length = min([len(split_reviews[i]) for i in range(0,5)])\n",
    "    for i, reviews in enumerate(split_reviews):\n",
    "        split_reviews[i] = split_reviews[i].sample(frac = 1)\n",
    "        split_reviews[i] = reviews.iloc[:length, :]\n",
    "    return merge_workbooks(split_reviews, column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Correcting contractions in English**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(raw):\n",
    "    \"\"\"\n",
    "        Removes contractions to clean sentences\n",
    "        \n",
    "        Paras:\n",
    "            raw: raw text data\n",
    "        Returns:\n",
    "            raw: cleaned text\n",
    "    \"\"\"\n",
    "    contractions = { \n",
    "                    \"ain't\": \"is not\",\n",
    "                    \"aren't\": \"are not\",\n",
    "                    \"can't\": \"cannot\",\n",
    "                    \"could've\": \"could have\",\n",
    "                    \"couldn't\": \"could not\",\n",
    "                    \"didn't\": \"did not\",\n",
    "                    \"doesn't\": \"does not\",\n",
    "                    \"don't\": \"do not\",\n",
    "                    \"hadn't\": \"had not\",\n",
    "                    \"hasn't\": \"has not\",\n",
    "                    \"haven't\": \"have not\",\n",
    "                    \"he'd\": \"he would\",\n",
    "                    \"he'll\": \"he will\",\n",
    "                    \"he's\": \"he is\",\n",
    "                    \"how'd\": \"how did\",\n",
    "                    \"how'll\": \"how will\",\n",
    "                    \"how's\": \"how is\",\n",
    "                    \"I'd\": \"I would\",\n",
    "                    \"I'll\": \"I will\",\n",
    "                    \"I'm\": \"I am\",\n",
    "                    \"I've\": \"I have\",\n",
    "                    \"isn't\": \"is not\",\n",
    "                    \"it'd\": \"it would\",\n",
    "                    \"it'll\": \"it will\",\n",
    "                    \"it's\": \"it is\",\n",
    "                    \"let's\": \"let us\",\n",
    "                    \"ma'am\": \"madam\",\n",
    "                    \"mayn't\": \"may not\",\n",
    "                    \"might've\": \"might have\",\n",
    "                    \"mightn't\": \"might not\",\n",
    "                    \"must've\": \"must have\",\n",
    "                    \"mustn't\": \"must not\",\n",
    "                    \"needn't\": \"need not\",\n",
    "                    \"o'clock\": \"of the clock\",\n",
    "                    \"oughtn't\": \"ought not\",\n",
    "                    \"shan't\": \"shall not\",\n",
    "                    \"sha'n't\": \"shall not\",\n",
    "                    \"she'd\": \"she would\",\n",
    "                    \"she'll\": \"she will\",\n",
    "                    \"she's\": \"she is\",\n",
    "                    \"should've\": \"should have\",\n",
    "                    \"shouldn't\": \"should not\",\n",
    "                    \"shouldn't've\": \"should not have\",\n",
    "                    \"so've\": \"so have\",\n",
    "                    \"so's\": \"so as\",\n",
    "                    \"that'd\": \"that would\",\n",
    "                    \"that's\": \"that is\",\n",
    "                    \"there'd\": \"there had\",\n",
    "                    \"there's\": \"there is\",\n",
    "                    \"they'd\": \"they would\",\n",
    "                    \"they'll\": \"they will\",\n",
    "                    \"they're\": \"they are\",\n",
    "                    \"they've\": \"they have\",\n",
    "                    \"to've\": \"to have\",\n",
    "                    \"wasn't\": \"was not\",\n",
    "                    \"we'd\": \"we would\",\n",
    "                    \"we'll\": \"we will\",\n",
    "                    \"we're\": \"we are\",\n",
    "                    \"we've\": \"we have\",\n",
    "                    \"weren't\": \"were not\",\n",
    "                    \"what'll\": \"what will\",\n",
    "                    \"what're\": \"what are\",\n",
    "                    \"what's\": \"what is\",\n",
    "                    \"what've\": \"what have\",\n",
    "                    \"when's\": \"when is\",\n",
    "                    \"when've\": \"when have\",\n",
    "                    \"where'd\": \"where did\",\n",
    "                    \"where's\": \"where is\",\n",
    "                    \"where've\": \"where have\",\n",
    "                    \"who'll\": \"who will\",\n",
    "                    \"who'll've\": \"who will have\",\n",
    "                    \"who's\": \"who is\",\n",
    "                    \"who've\": \"who have\",\n",
    "                    \"why's\": \"why has\",\n",
    "                    \"why've\": \"why have\",\n",
    "                    \"will've\": \"will have\",\n",
    "                    \"won't\": \"will not\",\n",
    "                    \"won't've\": \"will not have\",\n",
    "                    \"would've\": \"would have\",\n",
    "                    \"wouldn't\": \"would not\",\n",
    "                    \"y'all\": \"you all\",\n",
    "                    \"you'd\": \"you had / you would\",\n",
    "                    \"you'll\": \"you will\",\n",
    "                    \"you'll've\": \"you will have\",\n",
    "                    \"you're\": \"you are\",\n",
    "                    \"you've\": \"you have\"\n",
    "                }\n",
    "    if raw in contractions:\n",
    "        return re.sub(raw, contractions[raw], raw)\n",
    "    else:\n",
    "        return raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cleaning Text & Removing Noise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings\n",
    "\n",
    "        Paras:\n",
    "            text: text data to clean\n",
    "            remove_stopwords: if true, remove stop words from text to reduce noise\n",
    "        Returns:\n",
    "            text: cleaned text data\n",
    "    \"\"\"\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    text = text.encode(\"ascii\", errors = \"ignore\").decode()\n",
    "    text = [remove_contractions(word.lower()) for word in text.split()]\n",
    "    text = \" \".join([w for w in text if not w in stops])\n",
    "    return  re.sub(r\"[^a-zA-Z\\s+\\']\", \"\", text)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create training corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTrainingCorpus(df, name):\n",
    "    \"\"\"\n",
    "        Creates training data set with labels appended to beginging of each label\n",
    "\n",
    "        Paras:\n",
    "            df: datafframe\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    df = df.sample(frac = 1).reset_index(drop = True)\n",
    "    ratings = df[\"overall\"].tolist()\n",
    "    reviews = df[\"reviews\"].tolist()\n",
    "\n",
    "    for rating, review in zip(ratings, reviews):\n",
    "        doc =  \"__label__{}\".format(rating) + \" \" + review.strip()\n",
    "        doc = \" \".join([word for word in word_tokenize(doc) if len(word)>1])\n",
    "        save_data(\"../Dataset/training_processed\", \"{}.txt\".format(name), doc + \"\\n\", mode = \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create testing corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTestingCorpus(df, name):\n",
    "    \"\"\"\n",
    "        Constructs dataframe with test resutls\n",
    "\n",
    "        Paras:\n",
    "            None\n",
    "        Return:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    for _, temp in df.iterrows():\n",
    "        data = temp.overall + \" \" + temp.reviews + \"\\n\"\n",
    "        save_data(\"../Dataset/test_set/\", \"test_{}.txt\".format(name), data, mode = \"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set FastText Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setParameters(**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "        Parameters:\n",
    "            The following arguments are mandatory:\n",
    "            -input              training file path\n",
    "            -output             output file path\n",
    "\n",
    "            The following arguments are optional:\n",
    "            -verbose            verbosity level [2]\n",
    "\n",
    "            The following arguments for the dictionary are optional:\n",
    "            -minCount           minimal number of word occurrences [5]\n",
    "            -minCountLabel      minimal number of label occurrences [0]\n",
    "            -wordNgrams         max length of word ngram [1]\n",
    "            -bucket             number of buckets [2000000]\n",
    "            -minn               min length of char ngram [3]\n",
    "            -maxn               max length of char ngram [6]\n",
    "            -t                  sampling threshold [0.0001]\n",
    "            -label              labels prefix [__label__]\n",
    "\n",
    "            The following arguments for training are optional:\n",
    "            -lr                 learning rate [0.05]\n",
    "            -lrUpdateRate       change the rate of updates for the learning rate [100]\n",
    "            -dim                size of word vectors [100]\n",
    "            -ws                 size of the context window [5]\n",
    "            -epoch              number of epochs [5]\n",
    "            -neg                number of negatives sampled [5]\n",
    "            -loss               loss function {ns, hs, softmax} [ns]\n",
    "            -thread             number of threads [12]\n",
    "            -pretrainedVectors  pretrained word vectors for supervised learning []\n",
    "            -saveOutput         whether output params should be saved [0]\n",
    "\n",
    "            The following arguments for quantization are optional:\n",
    "            -cutoff             number of words and ngrams to retain [0]\n",
    "            -retrain            finetune embeddings if a cutoff is applied [0]\n",
    "            -qnorm              quantizing the norm separately [0]\n",
    "            -qout               quantizing the classifier [0]\n",
    "            -dsub               size of each sub-vector [2]\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    if kwargs[\"verbose\"] == None: verbose = None\n",
    "    else: verbose = \"-verbose {}\".format(kwargs[\"verbose\"])\n",
    "\n",
    "    if kwargs[\"minCount\"] == None: minCount = None\n",
    "    else: minCount = \"-minCount {}\".format(kwargs[\"minCount\"])\n",
    "\n",
    "    if kwargs[\"minCountLabel\"] == None: minCountLabel = None\n",
    "    else: minCountLabel = \"-minCountLabel {}\".format(kwargs[\"minCountLabel\"])\n",
    "\n",
    "    if kwargs[\"wordNgrams\"] == None: wordNgrams = None\n",
    "    else: wordNgrams = \"-wordNgrams {}\".format(kwargs[\"wordNgrams\"])\n",
    "\n",
    "    if kwargs[\"bucket\"] == None: bucket = None\n",
    "    else: bucket = \"-bucket {}\" .format(kwargs[\"bucket\"])\n",
    "\n",
    "    if kwargs[\"minn\"] == None: minn = None\n",
    "    else: minn = \"-minn {}\".format(kwargs[\"minn\"])\n",
    "\n",
    "    if kwargs[\"maxn\"] == None: maxn = None\n",
    "    else: maxn = \"-maxn {}\".format(kwargs[\"maxn\"])\n",
    "\n",
    "    if kwargs[\"t\"] == None: t = None\n",
    "    else: t = \"-t {}\".format(kwargs[\"t\"])\n",
    "\n",
    "    if kwargs[\"label\"] == None: label = None\n",
    "    else: label = \"-label {}\".format(kwargs[\"label\"])\n",
    "\n",
    "    if kwargs[\"lr\"] == None: lr = None\n",
    "    else: lr = \"-lr {}\".format(kwargs[\"lr\"])\n",
    "\n",
    "    if kwargs[\"lrUpdateRate\"] == None: lrUpdateRate = None\n",
    "    else: lrUpdateRate = \"-lrUpdateRate {}\".format(kwargs[\"lrUpdateRate\"])\n",
    "\n",
    "    if kwargs[\"dim\"] == None: dim = None\n",
    "    else: dim = \"-dim {}\".format(kwargs[\"dim\"])\n",
    "\n",
    "    if kwargs[\"ws\"] == None: ws = None\n",
    "    else: ws = \"-ws {}\".format(kwargs[\"ws\"])\n",
    "\n",
    "    if kwargs[\"epoch\"] == None: epoch = None\n",
    "    else: epoch = \"-epoch {}\".format(kwargs[\"epoch\"])\n",
    "\n",
    "    if kwargs[\"neg\"] == None: neg = None\n",
    "    else: neg = \"-neg {}\".format(kwargs[\"neg\"])\n",
    "\n",
    "    if kwargs[\"loss\"] == None: loss = None\n",
    "    else: loss = \"-loss {}\".format(kwargs[\"loss\"])\n",
    "\n",
    "    if kwargs[\"thread\"] == None: thread = None\n",
    "    else: thread = \"-thread {}\".format(kwargs[\"thread\"])\n",
    "\n",
    "    if kwargs[\"pretrainedVectors\"] == None: pretrainedVectors = None\n",
    "    else: pretrainedVectors = \"-pretrainedVectors {}\".format(kwargs[\"pretrainedVectors\"])\n",
    "\n",
    "    if kwargs[\"saveOutput\"] == None: saveOutput = None\n",
    "    else: saveOutput = \"-saveOutput {}\".format(kwargs[\"saveOutput\"])\n",
    "\n",
    "    if kwargs[\"cutoff\"] == None: cutoff = None\n",
    "    else: cutoff = \"-cutoff {}\".format(kwargs[\"cutoff\"])\n",
    "\n",
    "    if kwargs[\"retrain\"] == None: retrain = None\n",
    "    else: retrain = \"-retrain {}\".format(kwargs[\"retrain\"])\n",
    "\n",
    "    if kwargs[\"qnorm\"] == None: qnorm = None\n",
    "    else: qnorm = \"-qnorm {}\".format(kwargs[\"qnorm\"])\n",
    "\n",
    "    if kwargs[\"qout\"] == None: qout = None\n",
    "    else: qout = \"-qout {}\".format(kwargs[\"qout\"])\n",
    "\n",
    "    if kwargs[\"dsub\"] == None: dsub = None\n",
    "    else: dsub = \"-dsub {}\".format(kwargs[\"dsub\"])\n",
    "\n",
    "    cmd = [verbose, minCount, minCountLabel, wordNgrams, bucket, minn, maxn, t, label, lr, lrUpdateRate, dim, ws, epoch, neg, loss, thread, pretrainedVectors, saveOutput, cutoff, retrain, qnorm, qout, dsub]\n",
    "\n",
    "    return \" \".join(list(filter(None, cmd)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train FastText Supervised Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainClassifier(**kwargs):\n",
    "    \"\"\"\n",
    "        Trains supervised classifier\n",
    "        Paras:\n",
    "            hyper_parameters: parameters to train neural net\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    make_dir(\"../fastTextModels\")\n",
    "    name = kwargs[\"name\"]\n",
    "    model = kwargs[\"model\"]\n",
    "    parameters = setParameters(**kwargs)\n",
    "    system(\"./fastText/fasttext {} -input ./Dataset/training_set_processed/training_{}.txt -output ./fastTextModels/model_{} -label __label__ {}\".format(model, name, name, parameters))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test FastText Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestClassifier(name):\n",
    "    \"\"\"\n",
    "        Constructs dataframe with test resutls\n",
    "\n",
    "        Paras:\n",
    "            None\n",
    "        Return:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # if make_dir(\"../Dataset/testing/train.csv\"):\n",
    "\n",
    "    df = pd.read_csv(\"../Dataset/test_set/train.csv\")\n",
    "    df = df.loc[df[\"categoryName\"] == name]\n",
    "    df = remove_columns(df, [\"summary\", \"reviewText\", \"overall\"])\n",
    "\n",
    "    for _, temp in df.iterrows():\n",
    "        data = temp.summary + \". \" + temp.reviewText\n",
    "        data = clean_text(data)\n",
    "        data = temp.overall + \" \" + data + \"\\n\"\n",
    "        save_data(\"../Dataset/test_set/\", \"test_{}.txt\".format(name), data, mode = \"a\")\n",
    "\n",
    "    test_directory = join(\"../Dataset/test_set\", \"test_{}.txt\".format(name))\n",
    "    test_cmd = \"../fastText/fasttext test ../fastTextModels/model_{}.bin {}\".format(name, test_directory)\n",
    "    return shell2var(test_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"./Dataset/raw_training_set/CDVinyl.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asin</th>\n",
       "      <th>helpful</th>\n",
       "      <th>overall</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>reviewTime</th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0307141985</td>\n",
       "      <td>[14, 15]</td>\n",
       "      <td>5</td>\n",
       "      <td>I don't know who owns the rights to this wonde...</td>\n",
       "      <td>10 6, 2005</td>\n",
       "      <td>A3IEV6R2B7VW5Z</td>\n",
       "      <td>J. Anderson</td>\n",
       "      <td>LISTEN TO THE PUBLIC!!!</td>\n",
       "      <td>1128556800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0307141985</td>\n",
       "      <td>[38, 38]</td>\n",
       "      <td>5</td>\n",
       "      <td>This is a Thanksgiving tale that begins with t...</td>\n",
       "      <td>07 14, 2003</td>\n",
       "      <td>A6GMEO3VRY51S</td>\n",
       "      <td>microjoe</td>\n",
       "      <td>Thanksgiving Holiday fun from Rankin/Bass</td>\n",
       "      <td>1058140800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0307141985</td>\n",
       "      <td>[15, 16]</td>\n",
       "      <td>5</td>\n",
       "      <td>This is the BEST THANKSGIVING special around.....</td>\n",
       "      <td>11 6, 2003</td>\n",
       "      <td>A3E102F6LPUF1J</td>\n",
       "      <td>Richard J. Goldschmidt \"Rick Goldschmidt\"</td>\n",
       "      <td>BEST THANKSGIVING special out there!</td>\n",
       "      <td>1068076800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0307141985</td>\n",
       "      <td>[11, 12]</td>\n",
       "      <td>5</td>\n",
       "      <td>It's been a number of years since I've seen Mo...</td>\n",
       "      <td>03 1, 2006</td>\n",
       "      <td>A2JP0URFHXP6DO</td>\n",
       "      <td>Tim Janson</td>\n",
       "      <td>A THANKSGIVING TRADITION</td>\n",
       "      <td>1141171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[1, 16]</td>\n",
       "      <td>5</td>\n",
       "      <td>I read one of the review's on here for woodsto...</td>\n",
       "      <td>01 23, 2003</td>\n",
       "      <td>A3QAV7LALVG1F7</td>\n",
       "      <td>Dianne Papineau \"Brock Papineau\"</td>\n",
       "      <td>Future woodstock's will be better than the first</td>\n",
       "      <td>1043280000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[10, 12]</td>\n",
       "      <td>2</td>\n",
       "      <td>I paid, I went, I saw, I experienced.  Unfortu...</td>\n",
       "      <td>03 10, 2000</td>\n",
       "      <td>A1BFRIT70VHDF8</td>\n",
       "      <td>Doogie the Audio Junkie \"dackley\"</td>\n",
       "      <td>2% Of The Real Woodstock 99</td>\n",
       "      <td>952646400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>5</td>\n",
       "      <td>As a fan of filmed concerts, I avoided this on...</td>\n",
       "      <td>04 13, 2009</td>\n",
       "      <td>AEFGR6NFTNWH0</td>\n",
       "      <td>Emile Pinsonneault</td>\n",
       "      <td>Don't Get Hung Up On The \"Woodstock\" Thing...</td>\n",
       "      <td>1239580800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[11, 13]</td>\n",
       "      <td>4</td>\n",
       "      <td>I dock this DVD one star only because I wanted...</td>\n",
       "      <td>06 14, 2000</td>\n",
       "      <td>A24GD1AWG77IDJ</td>\n",
       "      <td>E. Uthman \"Ed\"</td>\n",
       "      <td>Many great performances; superb audio and video</td>\n",
       "      <td>960940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[2, 2]</td>\n",
       "      <td>1</td>\n",
       "      <td>This would be the perfect event to put on DVD....</td>\n",
       "      <td>10 21, 1999</td>\n",
       "      <td>A1THF19PO0LXRD</td>\n",
       "      <td>James Marsh</td>\n",
       "      <td>Where's the DVD???</td>\n",
       "      <td>940464000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>073890015X</td>\n",
       "      <td>[4, 5]</td>\n",
       "      <td>3</td>\n",
       "      <td>If you were to just read the names of the band...</td>\n",
       "      <td>02 18, 2006</td>\n",
       "      <td>A2KCJ6AACY6J0L</td>\n",
       "      <td>James Shannon Bussey</td>\n",
       "      <td>Good Bands, Bad Song Selection</td>\n",
       "      <td>1140220800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         asin   helpful  overall  \\\n",
       "0  0307141985  [14, 15]        5   \n",
       "1  0307141985  [38, 38]        5   \n",
       "2  0307141985  [15, 16]        5   \n",
       "3  0307141985  [11, 12]        5   \n",
       "4  073890015X   [1, 16]        5   \n",
       "5  073890015X  [10, 12]        2   \n",
       "6  073890015X    [0, 0]        5   \n",
       "7  073890015X  [11, 13]        4   \n",
       "8  073890015X    [2, 2]        1   \n",
       "9  073890015X    [4, 5]        3   \n",
       "\n",
       "                                          reviewText   reviewTime  \\\n",
       "0  I don't know who owns the rights to this wonde...   10 6, 2005   \n",
       "1  This is a Thanksgiving tale that begins with t...  07 14, 2003   \n",
       "2  This is the BEST THANKSGIVING special around.....   11 6, 2003   \n",
       "3  It's been a number of years since I've seen Mo...   03 1, 2006   \n",
       "4  I read one of the review's on here for woodsto...  01 23, 2003   \n",
       "5  I paid, I went, I saw, I experienced.  Unfortu...  03 10, 2000   \n",
       "6  As a fan of filmed concerts, I avoided this on...  04 13, 2009   \n",
       "7  I dock this DVD one star only because I wanted...  06 14, 2000   \n",
       "8  This would be the perfect event to put on DVD....  10 21, 1999   \n",
       "9  If you were to just read the names of the band...  02 18, 2006   \n",
       "\n",
       "       reviewerID                               reviewerName  \\\n",
       "0  A3IEV6R2B7VW5Z                                J. Anderson   \n",
       "1   A6GMEO3VRY51S                                   microjoe   \n",
       "2  A3E102F6LPUF1J  Richard J. Goldschmidt \"Rick Goldschmidt\"   \n",
       "3  A2JP0URFHXP6DO                                 Tim Janson   \n",
       "4  A3QAV7LALVG1F7           Dianne Papineau \"Brock Papineau\"   \n",
       "5  A1BFRIT70VHDF8          Doogie the Audio Junkie \"dackley\"   \n",
       "6   AEFGR6NFTNWH0                         Emile Pinsonneault   \n",
       "7  A24GD1AWG77IDJ                             E. Uthman \"Ed\"   \n",
       "8  A1THF19PO0LXRD                                James Marsh   \n",
       "9  A2KCJ6AACY6J0L                       James Shannon Bussey   \n",
       "\n",
       "                                            summary  unixReviewTime  \n",
       "0                           LISTEN TO THE PUBLIC!!!      1128556800  \n",
       "1         Thanksgiving Holiday fun from Rankin/Bass      1058140800  \n",
       "2              BEST THANKSGIVING special out there!      1068076800  \n",
       "3                          A THANKSGIVING TRADITION      1141171200  \n",
       "4  Future woodstock's will be better than the first      1043280000  \n",
       "5                       2% Of The Real Woodstock 99       952646400  \n",
       "6     Don't Get Hung Up On The \"Woodstock\" Thing...      1239580800  \n",
       "7   Many great performances; superb audio and video       960940800  \n",
       "8                                Where's the DVD???       940464000  \n",
       "9                    Good Bands, Bad Song Selection      1140220800  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 920,086 records containing customer reviews.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are {:,} records containing customer reviews.\".format(len(reviews)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of ratings over dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_bar(reviews, \"overall\", \"Ratings\", (15,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant imbalance in the distrubtion of the data, where the density of the positive labels are much more occurent than the negative labels. This will lead the model to be developed to be skewed towards positive ratings. To avoid creating a biased model, the dataset needs to be balance either by upsampling or downsampling the data. In this tutorial, the dataset will be down sampled, and the performance of the models will be compared accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Balancing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general approach to balancing the dataset begins with splitting the dataset into 5 sub dataframes. Each dataframe corresponds to numerical rating from 1 to 5. For Downsampling, the objective would be to down-size each dataframe to size of the dataframe that contains the least amount of labels. Therefore, creating equal number of labels across each dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Splitting Dataframes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are 5 target labels, the original dataframe is split accordingly in preparation for ingestion and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataframes = [reviews.loc[reviews[\"overall\"] == i] for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Downsampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"summary\", \"reviewText\", \"overall\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_reviews = DownSampleReviews(split_dataframes, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine summaries and reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a good amount of information that can be extracted from the summary section of each review. Therefore, it would be a good approach to combine the summaries with the reviews to trian the text classifier. Upon combining them, a new dataframe with the rating and the text data will be produced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_reviews[\"reviews\"] = balanced_reviews[\"summary\"] + \" \" + balanced_reviews[\"reviewText\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_reviews[\"reviews\"] = balanced_reviews[\"reviews\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews = balanced_reviews[[\"reviews\", \"overall\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training & Testing Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Randomly Shuffle Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_reviews_shuffled = processed_reviews.sample(frac = 1).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split data into training & testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_pct_len = int(0.8*len(processed_reviews_shuffled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = processed_reviews_shuffled.loc[:eight_pct_len, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = processed_reviews_shuffled.loc[eight_pct_len:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "createTrainingCorpus(training_data, \"CDVinyl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Testing Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"name\": \"CDVinyl\",\n",
    "    \"model\" :\"supervised\",\n",
    "    \"verbose\": None,\n",
    "    \"minCount\": None, \n",
    "    \"minCountLabel\": None, \n",
    "    \"wordNgrams\": None,\n",
    "    \"bucket\": None,\n",
    "    \"minn\": None,\n",
    "    \"maxn\": None, \n",
    "    \"t\": None, \n",
    "    \"label\": None, \n",
    "    \"lr\": None, \n",
    "    \"lrUpdateRate\": None, \n",
    "    \"dim\": None, \n",
    "    \"ws\": None, \n",
    "    \"epoch\": None, \n",
    "    \"neg\": None, \n",
    "    \"loss\": None, \n",
    "    \"thread\": None,\n",
    "    \"pretrainedVectors\": None, \n",
    "    \"saveOutput\": None, \n",
    "    \"cutoff\": None, \n",
    "    \"retrain\": None, \n",
    "    \"qnorm\": None, \n",
    "    \"qout\": None, \n",
    "    \"dsub\": None\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainClassifier(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
